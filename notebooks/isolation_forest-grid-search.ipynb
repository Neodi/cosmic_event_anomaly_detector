{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82007e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a528000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\___utad\\4_Cuarto\\TFG\\proyecto_PLAsTiCC\n"
     ]
    }
   ],
   "source": [
    "BASE            = Path.cwd().parents[0]  # or use Path(\"..\").resolve() if notebook is in a subfolder\n",
    "DATA_PROC       = BASE / \"data\" / \"processed\"\n",
    "NPZ_ROOT        = DATA_PROC / \"npz\"\n",
    "STATS_PATH      = DATA_PROC / \"stats.json\"\n",
    "CHECKPOINT      = BASE / \"logs\" / \"vae_experiment\" / \"version_2\" / \"checkpoints\" / \\\n",
    "                  \"vae-epoch=78-val_loss=0.0000.ckpt\"\n",
    "\n",
    "TRAIN_IDS_NPY   = DATA_PROC / \"train.npy\"\n",
    "VAL_IDS_NPY     = DATA_PROC / \"val.npy\"\n",
    "\n",
    "LAT_TRAIN_NPY   = DATA_PROC / \"latent_train.npy\"\n",
    "LAT_VAL_NPY     = DATA_PROC / \"latent_val.npy\"         \n",
    "\n",
    "MODEL_OUT       = BASE / \"models\" / \"iso_model.pkl\"\n",
    "print(f\"{BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH_JSON     = BASE / \"models\" / \"threshold_iso.json\"\n",
    "\n",
    "BATCH_SIZE      = 256\n",
    "N_WORKERS       = 4\n",
    "PROXY_RARE      = 2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3929a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import LightCurveDataset\n",
    "from train.model_vae import LitVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177d694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent(ids_npy: Path, out_npy: Path) -> np.ndarray:\n",
    "    \"\"\"Pasa los object_id del .npy por el encoder y guarda la matriz ¬µ.\"\"\"\n",
    "    if out_npy.exists():\n",
    "        print(f\"‚úì {out_npy.name} ya existe ‚Üí cargando\")\n",
    "        return np.load(out_npy)\n",
    "\n",
    "    print(f\"‚Üí Extrayendo latentes para {out_npy.stem}\")\n",
    "    # Modelo\n",
    "    model = LitVAE.load_from_checkpoint(str(CHECKPOINT))\n",
    "    model.eval().cuda()\n",
    "    latent_dim = model.hparams.latent_dim\n",
    "\n",
    "    # Stats para Dataset\n",
    "    stats = json.load(open(STATS_PATH))\n",
    "    flux_mean = torch.tensor(stats[\"flux\"][\"mean\"]).cuda()\n",
    "    flux_std  = torch.tensor(stats[\"flux\"][\"std\"]).cuda()\n",
    "    meta_keys = list(stats[\"meta\"].keys())\n",
    "    meta_mean = torch.tensor([stats[\"meta\"][k][\"mean\"] for k in meta_keys]).cuda()\n",
    "    std_meta  = torch.tensor([stats[\"meta\"][k][\"std\"]  for k in meta_keys]).cuda()\n",
    "\n",
    "    ids = np.load(ids_npy)\n",
    "    ds  = LightCurveDataset(ids, (flux_mean, flux_std, meta_mean, std_meta, meta_keys),\n",
    "                            root=NPZ_ROOT)\n",
    "    dl  = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                     num_workers=N_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    lat = np.zeros((len(ds), latent_dim), dtype=np.float32)\n",
    "    cur = 0\n",
    "    with torch.inference_mode():\n",
    "        for flux, mask, meta in tqdm(dl, desc=f\"Encoder({ids_npy.stem})\"):\n",
    "            lat[cur:cur+flux.size(0)] = model.encoder(flux.cuda())[0].cpu().numpy()\n",
    "            cur += flux.size(0)\n",
    "\n",
    "    np.save(out_npy, lat)\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b530992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì latent_train.npy ya existe ‚Üí cargando\n",
      "‚úì latent_val.npy ya existe ‚Üí cargando\n"
     ]
    }
   ],
   "source": [
    "X_train = extract_latent(TRAIN_IDS_NPY, LAT_TRAIN_NPY)\n",
    "X_val   = extract_latent(VAL_IDS_NPY,   LAT_VAL_NPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdab6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # 2) Estandarizar con stats del train\n",
    "scaler  = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std   = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae8afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Etiquetas proxy raras en validation\n",
    "meta_df   = pd.read_csv(\"../data/raw/plasticc_train_metadata.csv\", usecols=[\"object_id\", \"target\"])\n",
    "freqs     = meta_df[\"target\"].value_counts().sort_values()\n",
    "rare_set  = set(freqs.index[:PROXY_RARE])\n",
    "val_ids   = np.load(VAL_IDS_NPY)\n",
    "meta_dict = dict(zip(meta_df[\"object_id\"].values, meta_df[\"target\"].values))\n",
    "y_true_val = np.array([1 if meta_dict.get(int(i), -1) in rare_set else 0 for i in val_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "GRID_PARAMS     = {\n",
    "    \"n_estimators\":  [100, 200, 300],\n",
    "    \"max_samples\":   [\"auto\", 0.5, 0.7, 0.9],\n",
    "    \"contamination\": [0.01, 0.03, 0.05, 0.08, 0.1],\n",
    "    \"max_features\":  [0.5, 0.7, 0.9, 1.0],\n",
    "    \"bootstrap\":     [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3188dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando grid search con 480 combinaciones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [28:34<00:00,  3.57s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Mejor AUC=0.472 con params (100, 0.9, 0.01, 1.0, True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Grid-search Isolation Forest\n",
    "best_auc, best_iso, best_params = -1, None, None\n",
    "\n",
    "# Calcular total de combinaciones para mostrar progreso\n",
    "total_combinations = len(GRID_PARAMS[\"n_estimators\"]) * len(GRID_PARAMS[\"max_samples\"]) * \\\n",
    "                    len(GRID_PARAMS[\"contamination\"]) * len(GRID_PARAMS[\"max_features\"]) * \\\n",
    "                    len(GRID_PARAMS[\"bootstrap\"])\n",
    "\n",
    "print(f\"Iniciando grid search con {total_combinations} combinaciones...\")\n",
    "\n",
    "for i, (n_estim, max_samp, contam, max_feat, bootstrap) in enumerate(\n",
    "    tqdm(product(GRID_PARAMS[\"n_estimators\"],\n",
    "                 GRID_PARAMS[\"max_samples\"],\n",
    "                 GRID_PARAMS[\"contamination\"],\n",
    "                 GRID_PARAMS[\"max_features\"],\n",
    "                 GRID_PARAMS[\"bootstrap\"]), \n",
    "         total=total_combinations,\n",
    "         desc=\"Grid Search\")):\n",
    "    \n",
    "    iso = IsolationForest(\n",
    "        n_estimators=n_estim, max_samples=max_samp,\n",
    "        contamination=contam, max_features=max_feat, bootstrap=bootstrap,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ).fit(X_train_std)\n",
    "\n",
    "    scores_val = -iso.score_samples(X_val_std)  # signo invertido (m√°s grande = outlier)\n",
    "    auc = roc_auc_score(y_true_val, scores_val)\n",
    "\n",
    "    # print(f\"IF {n_estim=} {max_samp=} {contam=} {max_feat=} {bootstrap=} ‚Üí AUC={auc:.3f}\")\n",
    "    if auc > best_auc:\n",
    "        best_auc, best_iso, best_params = auc, iso, (n_estim, max_samp, contam, max_feat, bootstrap)\n",
    "\n",
    "print(f\"\\nüèÜ Mejor AUC={best_auc:.3f} con params {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64560a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b5cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbrales calculados:\n",
      "score_threshold_p1 = 0.3034\n",
      "score_threshold_p3 = 0.3035\n",
      "score_threshold_p5 = 0.3035\n",
      "score_threshold_p10 = 0.3036\n"
     ]
    }
   ],
   "source": [
    "# 5) Umbral = varios percentiles de score sobre train\n",
    "scores_train = -best_iso.score_samples(X_train_std)  # signo invertido para consistencia\n",
    "thresholds = {\n",
    "    \"score_threshold_p1\": float(np.percentile(scores_train, 1)),    # percentil 1%\n",
    "    \"score_threshold_p3\": float(np.percentile(scores_train, 3)),    # percentil 3%\n",
    "    \"score_threshold_p5\": float(np.percentile(scores_train, 5)),    # percentil 5%\n",
    "    \"score_threshold_p10\": float(np.percentile(scores_train, 10)),  # percentil 10%\n",
    "}\n",
    "\n",
    "print(\"Umbrales calculados:\")\n",
    "for key, value in thresholds.items():\n",
    "    print(f\"{key} = {value:.4f}\")\n",
    "\n",
    "# Mantener thr_p5 para compatibilidad con el c√≥digo siguiente\n",
    "thr_p5 = thresholds[\"score_threshold_p5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5aa871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8fe90e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo IsolationForest guardado en c:\\Users\\David\\___utad\\4_Cuarto\\TFG\\proyecto_PLAsTiCC\\models\\iso_model.pkl\n",
      "‚úÖ Umbral guardado en c:\\Users\\David\\___utad\\4_Cuarto\\TFG\\proyecto_PLAsTiCC\\models\\threshold_iso.json\n"
     ]
    }
   ],
   "source": [
    "# 6) Guardar modelo + scaler + umbral\n",
    "MODEL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump({\"iso\": best_iso, \"scaler\": scaler}, MODEL_OUT)\n",
    "with open(THRESH_JSON, \"w\") as f:\n",
    "    json.dump(thresholds, f, indent=4)\n",
    "print(f\"‚úÖ Modelo IsolationForest guardado en {MODEL_OUT}\")\n",
    "print(f\"‚úÖ Umbral guardado en {THRESH_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plasticc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
